{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73c7a35bf7aec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common utilities for web search\n",
    "from duckduckgo_search import DDGS\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class WebSearchTool:\n",
    "    def search(self, query: str, num_results: int = 5) -> List[Dict]:\n",
    "        with DDGS() as ddgs:\n",
    "            results = list(ddgs.text(query, max_results=num_results))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a27a7868204556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Implementation\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "\n",
    "class OpenAIResearchAgent:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.web_search = WebSearchTool()\n",
    "\n",
    "        # Define custom functions for the assistant\n",
    "        self.tools = [{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_internet\",\n",
    "                \"description\": \"Search the internet for information\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query\"\n",
    "                        },\n",
    "                        \"num_results\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"Number of results to return\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "\n",
    "        # Create an assistant\n",
    "        self.assistant = self.client.beta.assistants.create(\n",
    "            name=\"Research Agent\",\n",
    "            instructions=\"\"\"You are a research agent that:\n",
    "                1. Searches the internet for current information\n",
    "                2. Analyzes and synthesizes found information\n",
    "                3. Provides detailed summaries with citations\n",
    "                4. Suggests follow-up research areas\n",
    "                When searching, break down complex queries into specific searchable terms.\"\"\",\n",
    "            model=\"gpt-4-turbo\",\n",
    "            # model=\"gpt-4-turbo-preview\",\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "    def search_internet(self, query: str, num_results: int = 5) -> str:\n",
    "        results = self.web_search.search(query, num_results)\n",
    "        return json.dumps(results)\n",
    "\n",
    "    def start_research(self, query: str) -> str:\n",
    "        # Create a thread\n",
    "        thread = self.client.beta.threads.create()\n",
    "\n",
    "        # Add the initial message\n",
    "        # message = self.client.beta.threads.messages.create(\n",
    "        #     thread_id=thread.id,\n",
    "        #     role=\"user\",\n",
    "        #     content=query\n",
    "        # )\n",
    "\n",
    "        # Run the assistant\n",
    "        run = self.client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=self.assistant.id\n",
    "        )\n",
    "\n",
    "        # Handle tool calls\n",
    "        while run.status in [\"queued\", \"in_progress\"]:\n",
    "            run = self.client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id\n",
    "            )\n",
    "\n",
    "            if run.status == \"requires_action\":\n",
    "                tool_calls = run.required_action.submit_tool_outputs.tool_calls\n",
    "                tool_outputs = []\n",
    "\n",
    "                for tool_call in tool_calls:\n",
    "                    if tool_call.function.name == \"search_internet\":\n",
    "                        args = json.loads(tool_call.function.arguments)\n",
    "                        output = self.search_internet(args[\"query\"], args.get(\"num_results\", 5))\n",
    "                        tool_outputs.append({\n",
    "                            \"tool_call_id\": tool_call.id,\n",
    "                            \"output\": output\n",
    "                        })\n",
    "\n",
    "                run = self.client.beta.threads.runs.submit_tool_outputs(\n",
    "                    thread_id=thread.id,\n",
    "                    run_id=run.id,\n",
    "                    tool_outputs=tool_outputs\n",
    "                )\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "        # Get the final response\n",
    "        messages = self.client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        return messages.data[0].content[0].text.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb29415651b407e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anthropic Implementation\n",
    "from anthropic import Anthropic\n",
    "\n",
    "class AnthropicResearchAgent:\n",
    "    def __init__(self, api_key):\n",
    "        self.client = Anthropic(api_key=api_key)\n",
    "        self.web_search = WebSearchTool()\n",
    "        self.memory = []\n",
    "\n",
    "    def _create_system_prompt(self):\n",
    "        return \"\"\"You are a research agent that systematically:\n",
    "            1. Searches the internet for current information\n",
    "            2. Analyzes and synthesizes search results\n",
    "            3. Maintains context across interactions\n",
    "            4. Provides citations for all findings\n",
    "            5. Suggests related research directions\"\"\"\n",
    "\n",
    "    def process_query(self, query: str) -> str:\n",
    "        # First, perform a web search\n",
    "        search_results = self.web_search.search(query)\n",
    "\n",
    "        # Create messages with memory and search results\n",
    "        messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": self._create_system_prompt()\n",
    "        }] + self.memory + [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"Query: {query}\n",
    "\n",
    "            Search Results:\n",
    "            {json.dumps(search_results, indent=2)}\n",
    "\n",
    "            Please analyze these results and provide a comprehensive response.\"\"\"\n",
    "        }]\n",
    "\n",
    "        # Get response using Claude\n",
    "        response = self.client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\",\n",
    "            max_tokens=2000,\n",
    "            messages=messages,\n",
    "            tools=[{\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"perform_additional_search\",\n",
    "                    \"description\": \"Perform additional internet search\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"query\": {\"type\": \"string\"},\n",
    "                            \"num_results\": {\"type\": \"integer\"}\n",
    "                        },\n",
    "                        \"required\": [\"query\"]\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        )\n",
    "\n",
    "        # Update memory\n",
    "        self.memory.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        })\n",
    "        self.memory.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": response.content[0].text\n",
    "        })\n",
    "\n",
    "        return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "393751a60d2b44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini Implementation\n",
    "# import google.generativeai as genai\n",
    "from google import genai\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "class GeminiResearchAgent:\n",
    "    def __init__(self, api_key):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "        self.chat = self.model.start_chat(history=[])\n",
    "        self.web_search = WebSearchTool()\n",
    "        self.setup_database()\n",
    "        self.conn = None\n",
    "\n",
    "    def setup_database(self):\n",
    "        self.conn = sqlite3.connect('research_memory.db')\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS research_memory\n",
    "            (timestamp TEXT, topic TEXT, findings TEXT, sources TEXT)\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def store_memory(self, topic: str, findings: str, sources: List[str]):\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        self.cursor.execute(\n",
    "            'INSERT INTO research_memory VALUES (?, ?, ?, ?)',\n",
    "            (timestamp, topic, findings, json.dumps(sources))\n",
    "        )\n",
    "        self.conn.commit()\n",
    "\n",
    "    def get_related_memories(self, topic: str) -> List[Dict]:\n",
    "        self.cursor.execute(\n",
    "            'SELECT findings, sources FROM research_memory WHERE topic LIKE ?',\n",
    "            (f'%{topic}%',)\n",
    "        )\n",
    "        results = []\n",
    "        for row in self.cursor.fetchall():\n",
    "            results.append({\n",
    "                'findings': row[0],\n",
    "                'sources': json.loads(row[1])\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    def research_topic(self, query: str) -> str:\n",
    "        # Perform web search\n",
    "        search_results = self.web_search.search(query)\n",
    "\n",
    "        # Get related previous research\n",
    "        related_memories = self.get_related_memories(query)\n",
    "\n",
    "        # Create context-aware prompt\n",
    "        context = f\"\"\"Research Query: {query}\n",
    "\n",
    "        New Search Results:\n",
    "        {json.dumps(search_results, indent=2)}\n",
    "\n",
    "        Previous Related Research:\n",
    "        {\n",
    "            json.dumps(related_memories, indent=2) \n",
    "            if related_memories \n",
    "            else 'No previous research found.'\n",
    "        }\n",
    "\n",
    "        Please analyze this topic considering:\n",
    "        1. Key findings from search results\n",
    "        2. Integration with previous research\n",
    "        3. Practical applications\n",
    "        4. Future implications\n",
    "\n",
    "        Provide citations for all information.\"\"\"\n",
    "\n",
    "        # Get response from Gemini\n",
    "        response = self.chat.send_message({\n",
    "            'contents': [{\n",
    "                'role': \"user\",\n",
    "                'parts': [{'text': context}]\n",
    "            }],\n",
    "            'tools': [{\n",
    "                'name': 'research_analysis',\n",
    "                'parameters': {\n",
    "                    'depth': 'comprehensive',\n",
    "                    'focus': 'technical and practical'\n",
    "                }\n",
    "            }]\n",
    "        })\n",
    "\n",
    "        # Store the findings with sources\n",
    "        self.store_memory(\n",
    "            query,\n",
    "            response.text,\n",
    "            [result['link'] for result in search_results]\n",
    "        )\n",
    "\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc4184d8065e19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for each agent\n",
    "def demonstrate_agents():\n",
    "    # OpenAI example\n",
    "    openai_agent = OpenAIResearchAgent(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    openai_result = openai_agent.start_research(\n",
    "        \"What are the latest developments in artificial intelligence research?\"\n",
    "    )\n",
    "    print(\"OpenAI Agent Results:\", openai_result)\n",
    "\n",
    "    # Anthropic example\n",
    "    # anthropic_agent = AnthropicResearchAgent(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
    "    # anthropic_result = anthropic_agent.process_query(\n",
    "    #     \"What are the current challenges in quantum computing?\"\n",
    "    # )\n",
    "    # print(\"Anthropic Agent Results:\", anthropic_result)\n",
    "\n",
    "    # Gemini example\n",
    "    # gemini_agent = GeminiResearchAgent(os.environ.get(\"GOOGLE_GEMINI_API_KEY\"))\n",
    "    # gemini_result = gemini_agent.research_topic(\n",
    "    #     \"How is AI being used in climate change mitigation?\"\n",
    "    # )\n",
    "    # print(\"Gemini Agent Results:\", gemini_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f60e5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Agent Results: ### Mutual Funds with the Highest Returns in 2023\n",
      "\n",
      "The information I've gathered highlights specifics about mutual funds that have shown the highest returns in 2023, including performance in different categories such as large cap, mid-cap, and small-cap funds.\n",
      "\n",
      "1. **Average Annual Returns**:\n",
      "   - Large Cap Mutual Funds: 16.15%\n",
      "   - Mid Cap Mutual Funds: 30.77%\n",
      "   - Small Cap Mutual Funds: 34.29%\n",
      "   \n",
      "   These averages indicate a significant performance difference across the spectrum of fund sizes, with smaller cap funds outperforming their larger counterparts. [Source](https://www.hindustantimes.com/business/5-mutual-funds-that-gave-highest-returns-in-2023-mahindra-nippon-hdfc-101704276858101.html)\n",
      "\n",
      "2. **Top Performers in Each Category**:\n",
      "   - **Large Cap Mutual Funds**:\n",
      "     - XYZ Blue Chip Growth: 18.45%\n",
      "     - ABC Titan Stability: 17.89%\n",
      "   \n",
      "   Although specific fund names like \"XYZ Blue Chip Growth\" and \"ABC Titan Stability\" are given as top performers, these appear to be placeholders and not actual fund names. The precise details on leading funds in each category can provide a more actionable guide for investors seeking high returns. [Source](https://www.goodreturns.in/personal-finance/here-is-a-list-of-mutual-funds-that-gave-maximum-returns-in-2023-small-caps-lead-1320109.html)\n",
      "\n",
      "3. **Screening and Analysis**:\n",
      "   - Various sources, like Forbes Advisor and Investor's Business Daily, continue exploring methods to identify top performing mutual funds, emphasizing a rigorous screening process from the thousands available. [Forbes Advisor Article](https://www.forbes.com/advisor/investing/best-mutual-funds/)\n",
      "\n",
      "4. **Trends and Key Takeaways**:\n",
      "   - The mutual fund industry's performance data for 2023 illustrates a trend where smaller cap funds generally offer higher returns than their large-cap counterparts. It also highlights the competitive and evolving nature of mutual fund performance, urging investors to pay close attention to fund management strategies and market trends.\n",
      "\n",
      "### Follow-Up Research Areas\n",
      "\n",
      "Given the initial findings, relevant areas for further exploration could include:\n",
      "\n",
      "1. **Detailed Performance Analysis**: A closer examination of the top-performing mutual funds, focusing on their management teams, investment strategies, and the factors contributing to their success.\n",
      "   \n",
      "2. **Risk Assessment**: Understanding the risk profiles associated with the highest-performing mutual funds, especially considering the volatility typically associated with high returns, particularly in small and mid-cap funds.\n",
      "\n",
      "3. **Comparison with Previous Years**: Comparing the performance of mutual funds in 2023 with previous years to identify trends, such as shifts in top-performing fund categories or consistent performers over the years.\n",
      "\n",
      "4. **Impact of Economic Conditions**: Analyzing how prevailing economic conditions in 2023, such as inflation rates or market volatility, have influenced mutual fund performances, and identifying funds that have managed to thrive in spite of challenging economic circumstances.\n",
      "\n",
      "This analysis aims to provide a foundational understanding of mutual fund performance in 2023, highlighting areas for further study and considerations for investors looking to enhance their portfolios.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demonstrate_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627dceb5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
