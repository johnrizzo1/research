{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Empirical Study of Autoregressive Pre-training from Videos\n",
      "We empirically study autoregressive pre-training from videos. To perform our\n",
      "study, we construct a series of autoregressive video models, called Toto. We\n",
      "treat videos as sequences of visual tokens and train transformer models to\n",
      "autoregressively predict future tokens. Our models are pre-trained on a diverse\n",
      "dataset of videos and images comprising over 1 trillion visual tokens. We\n",
      "explore different architectural, training, and inference design choices. We\n",
      "evaluate the learned visual representations on a range of downstream tasks\n",
      "including image recognition, video classification, object tracking, and\n",
      "robotics. Our results demonstrate that, despite minimal inductive biases,\n",
      "autoregressive pre-training leads to competitive performance across all\n",
      "benchmarks. Finally, we find that scaling our video models results in similar\n",
      "scaling curves to those seen in language models, albeit with a different rate.\n",
      "More details at https://brjathu.github.io/toto/\n",
      "2025-01-09 18:59:58+00:00\n",
      "[arxiv.Result.Author('Jathushan Rajasegaran'), arxiv.Result.Author('Ilija Radosavovic'), arxiv.Result.Author('Rahul Ravishankar'), arxiv.Result.Author('Yossi Gandelsman'), arxiv.Result.Author('Christoph Feichtenhofer'), arxiv.Result.Author('Jitendra Malik')]\n",
      "http://arxiv.org/abs/2501.05453v1\n",
      "---\n",
      "Decentralized Diffusion Models\n",
      "Large-scale AI model training divides work across thousands of GPUs, then\n",
      "synchronizes gradients across them at each step. This incurs a significant\n",
      "network burden that only centralized, monolithic clusters can support, driving\n",
      "up infrastructure costs and straining power systems. We propose Decentralized\n",
      "Diffusion Models, a scalable framework for distributing diffusion model\n",
      "training across independent clusters or datacenters by eliminating the\n",
      "dependence on a centralized, high-bandwidth networking fabric. Our method\n",
      "trains a set of expert diffusion models over partitions of the dataset, each in\n",
      "full isolation from one another. At inference time, the experts ensemble\n",
      "through a lightweight router. We show that the ensemble collectively optimizes\n",
      "the same objective as a single model trained over the whole dataset. This means\n",
      "we can divide the training burden among a number of \"compute islands,\" lowering\n",
      "infrastructure costs and improving resilience to localized GPU failures.\n",
      "Decentralized diffusion models empower researchers to take advantage of\n",
      "smaller, more cost-effective and more readily available compute like on-demand\n",
      "GPU nodes rather than central integrated systems. We conduct extensive\n",
      "experiments on ImageNet and LAION Aesthetics, showing that decentralized\n",
      "diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally\n",
      "scale our approach to 24 billion parameters, demonstrating that high-quality\n",
      "diffusion models can now be trained with just eight individual GPU nodes in\n",
      "less than a week.\n",
      "2025-01-09 18:59:56+00:00\n",
      "[arxiv.Result.Author('David McAllister'), arxiv.Result.Author('Matthew Tancik'), arxiv.Result.Author('Jiaming Song'), arxiv.Result.Author('Angjoo Kanazawa')]\n",
      "http://arxiv.org/abs/2501.05450v1\n",
      "---\n",
      "Explainable AI-Enhanced Deep Learning for Pumpkin Leaf Disease Detection: A Comparative Analysis of CNN Architectures\n",
      "Pumpkin leaf diseases are significant threats to agricultural productivity,\n",
      "requiring a timely and precise diagnosis for effective management. Traditional\n",
      "identification methods are laborious and susceptible to human error,\n",
      "emphasizing the necessity for automated solutions. This study employs on the\n",
      "\"Pumpkin Leaf Disease Dataset\", that comprises of 2000 high-resolution images\n",
      "separated into five categories. Downy mildew, powdery mildew, mosaic disease,\n",
      "bacterial leaf spot, and healthy leaves. The dataset was rigorously assembled\n",
      "from several agricultural fields to ensure a strong representation for model\n",
      "training. We explored many proficient deep learning architectures, including\n",
      "DenseNet201, DenseNet121, DenseNet169, Xception, ResNet50, ResNet101 and\n",
      "InceptionResNetV2, and observed that ResNet50 performed most effectively, with\n",
      "an accuracy of 90.5% and comparable precision, recall, and F1-Score. We used\n",
      "Explainable AI (XAI) approaches like Grad-CAM, Grad-CAM++, Score-CAM, and\n",
      "Layer-CAM to provide meaningful representations of model decision-making\n",
      "processes, which improved understanding and trust in automated disease\n",
      "diagnostics. These findings demonstrate ResNet50's potential to revolutionize\n",
      "pumpkin leaf disease detection, allowing for earlier and more accurate\n",
      "treatments.\n",
      "2025-01-09 18:59:35+00:00\n",
      "[arxiv.Result.Author('Md. Arafat Alam Khandaker'), arxiv.Result.Author('Ziyan Shirin Raha'), arxiv.Result.Author('Shifat Islam'), arxiv.Result.Author('Tashreef Muhammad')]\n",
      "http://arxiv.org/abs/2501.05449v1\n",
      "---\n",
      "Consistent Flow Distillation for Text-to-3D Generation\n",
      "Score Distillation Sampling (SDS) has made significant strides in distilling\n",
      "image-generative models for 3D generation. However, its\n",
      "maximum-likelihood-seeking behavior often leads to degraded visual quality and\n",
      "diversity, limiting its effectiveness in 3D applications. In this work, we\n",
      "propose Consistent Flow Distillation (CFD), which addresses these limitations.\n",
      "We begin by leveraging the gradient of the diffusion ODE or SDE sampling\n",
      "process to guide the 3D generation. From the gradient-based sampling\n",
      "perspective, we find that the consistency of 2D image flows across different\n",
      "viewpoints is important for high-quality 3D generation. To achieve this, we\n",
      "introduce multi-view consistent Gaussian noise on the 3D object, which can be\n",
      "rendered from various viewpoints to compute the flow gradient. Our experiments\n",
      "demonstrate that CFD, through consistent flows, significantly outperforms\n",
      "previous methods in text-to-3D generation.\n",
      "2025-01-09 18:56:05+00:00\n",
      "[arxiv.Result.Author('Runjie Yan'), arxiv.Result.Author('Yinbo Chen'), arxiv.Result.Author('Xiaolong Wang')]\n",
      "http://arxiv.org/abs/2501.05445v1\n",
      "---\n",
      "The GAN is dead; long live the GAN! A Modern GAN Baseline\n",
      "There is a widely-spread claim that GANs are difficult to train, and GAN\n",
      "architectures in the literature are littered with empirical tricks. We provide\n",
      "evidence against this claim and build a modern GAN baseline in a more\n",
      "principled manner. First, we derive a well-behaved regularized relativistic GAN\n",
      "loss that addresses issues of mode dropping and non-convergence that were\n",
      "previously tackled via a bag of ad-hoc tricks. We analyze our loss\n",
      "mathematically and prove that it admits local convergence guarantees, unlike\n",
      "most existing relativistic losses. Second, our new loss allows us to discard\n",
      "all ad-hoc tricks and replace outdated backbones used in common GANs with\n",
      "modern architectures. Using StyleGAN2 as an example, we present a roadmap of\n",
      "simplification and modernization that results in a new minimalist baseline --\n",
      "R3GAN. Despite being simple, our approach surpasses StyleGAN2 on FFHQ,\n",
      "ImageNet, CIFAR, and Stacked MNIST datasets, and compares favorably against\n",
      "state-of-the-art GANs and diffusion models.\n",
      "2025-01-09 18:53:06+00:00\n",
      "[arxiv.Result.Author('Yiwen Huang'), arxiv.Result.Author('Aaron Gokaslan'), arxiv.Result.Author('Volodymyr Kuleshov'), arxiv.Result.Author('James Tompkin')]\n",
      "http://arxiv.org/abs/2501.05441v1\n",
      "---\n",
      "From Simple to Complex Skills: The Case of In-Hand Object Reorientation\n",
      "Learning policies in simulation and transferring them to the real world has\n",
      "become a promising approach in dexterous manipulation. However, bridging the\n",
      "sim-to-real gap for each new task requires substantial human effort, such as\n",
      "careful reward engineering, hyperparameter tuning, and system identification.\n",
      "In this work, we present a system that leverages low-level skills to address\n",
      "these challenges for more complex tasks. Specifically, we introduce a\n",
      "hierarchical policy for in-hand object reorientation based on previously\n",
      "acquired rotation skills. This hierarchical policy learns to select which\n",
      "low-level skill to execute based on feedback from both the environment and the\n",
      "low-level skill policies themselves. Compared to learning from scratch, the\n",
      "hierarchical policy is more robust to out-of-distribution changes and transfers\n",
      "easily from simulation to real-world environments. Additionally, we propose a\n",
      "generalizable object pose estimator that uses proprioceptive information,\n",
      "low-level skill predictions, and control errors as inputs to estimate the\n",
      "object pose over time. We demonstrate that our system can reorient objects,\n",
      "including symmetrical and textureless ones, to a desired pose.\n",
      "2025-01-09 18:49:39+00:00\n",
      "[arxiv.Result.Author('Haozhi Qi'), arxiv.Result.Author('Brent Yi'), arxiv.Result.Author('Mike Lambeta'), arxiv.Result.Author('Yi Ma'), arxiv.Result.Author('Roberto Calandra'), arxiv.Result.Author('Jitendra Malik')]\n",
      "http://arxiv.org/abs/2501.05439v1\n",
      "---\n",
      "Neuro-Symbolic AI in 2024: A Systematic Review\n",
      "Background: The field of Artificial Intelligence has undergone cyclical\n",
      "periods of growth and decline, known as AI summers and winters. Currently, we\n",
      "are in the third AI summer, characterized by significant advancements and\n",
      "commercialization, particularly in the integration of Symbolic AI and\n",
      "Sub-Symbolic AI, leading to the emergence of Neuro-Symbolic AI.\n",
      "  Methods: The review followed the PRISMA methodology, utilizing databases such\n",
      "as IEEE Explore, Google Scholar, arXiv, ACM, and SpringerLink. The inclusion\n",
      "criteria targeted peer-reviewed papers published between 2020 and 2024. Papers\n",
      "were screened for relevance to Neuro-Symbolic AI, with further inclusion based\n",
      "on the availability of associated codebases to ensure reproducibility.\n",
      "  Results: From an initial pool of 1,428 papers, 167 met the inclusion criteria\n",
      "and were analyzed in detail. The majority of research efforts are concentrated\n",
      "in the areas of learning and inference (63%), logic and reasoning (35%), and\n",
      "knowledge representation (44%). Explainability and trustworthiness are less\n",
      "represented (28%), with Meta-Cognition being the least explored area (5%). The\n",
      "review identifies significant interdisciplinary opportunities, particularly in\n",
      "integrating explainability and trustworthiness with other research areas.\n",
      "  Conclusion: Neuro-Symbolic AI research has seen rapid growth since 2020, with\n",
      "concentrated efforts in learning and inference. Significant gaps remain in\n",
      "explainability, trustworthiness, and Meta-Cognition. Addressing these gaps\n",
      "through interdisciplinary research will be crucial for advancing the field\n",
      "towards more intelligent, reliable, and context-aware AI systems.\n",
      "2025-01-09 18:48:35+00:00\n",
      "[arxiv.Result.Author('Brandon C. Colelough'), arxiv.Result.Author('William Regli')]\n",
      "http://arxiv.org/abs/2501.05435v1\n",
      "---\n",
      "From Images to Insights: Transforming Brain Cancer Diagnosis with Explainable AI\n",
      "Brain cancer represents a major challenge in medical diagnostics, requisite\n",
      "precise and timely detection for effective treatment. Diagnosis initially\n",
      "relies on the proficiency of radiologists, which can cause difficulties and\n",
      "threats when the expertise is sparse. Despite the use of imaging resources,\n",
      "brain cancer remains often difficult, time-consuming, and vulnerable to\n",
      "intraclass variability. This study conveys the Bangladesh Brain Cancer MRI\n",
      "Dataset, containing 6,056 MRI images organized into three categories: Brain\n",
      "Tumor, Brain Glioma, and Brain Menin. The dataset was collected from several\n",
      "hospitals in Bangladesh, providing a diverse and realistic sample for research.\n",
      "We implemented advanced deep learning models, and DenseNet169 achieved\n",
      "exceptional results, with accuracy, precision, recall, and F1-Score all\n",
      "reaching 0.9983. In addition, Explainable AI (XAI) methods including GradCAM,\n",
      "GradCAM++, ScoreCAM, and LayerCAM were employed to provide visual\n",
      "representations of the decision-making processes of the models. In the context\n",
      "of brain cancer, these techniques highlight DenseNet169's potential to enhance\n",
      "diagnostic accuracy while simultaneously offering transparency, facilitating\n",
      "early diagnosis and better patient outcomes.\n",
      "2025-01-09 18:35:43+00:00\n",
      "[arxiv.Result.Author('Md. Arafat Alam Khandaker'), arxiv.Result.Author('Ziyan Shirin Raha'), arxiv.Result.Author('Salehin Bin Iqbal'), arxiv.Result.Author('M. F. Mridha'), arxiv.Result.Author('Jungpil Shin')]\n",
      "http://arxiv.org/abs/2501.05426v1\n",
      "---\n",
      "Entangled Mean Estimation in High-Dimensions\n",
      "We study the task of high-dimensional entangled mean estimation in the\n",
      "subset-of-signals model. Specifically, given $N$ independent random points\n",
      "$x_1,\\ldots,x_N$ in $\\mathbb{R}^D$ and a parameter $\\alpha \\in (0, 1)$ such\n",
      "that each $x_i$ is drawn from a Gaussian with mean $\\mu$ and unknown\n",
      "covariance, and an unknown $\\alpha$-fraction of the points have\n",
      "identity-bounded covariances, the goal is to estimate the common mean $\\mu$.\n",
      "The one-dimensional version of this task has received significant attention in\n",
      "theoretical computer science and statistics over the past decades. Recent work\n",
      "[LY20; CV24] has given near-optimal upper and lower bounds for the\n",
      "one-dimensional setting. On the other hand, our understanding of even the\n",
      "information-theoretic aspects of the multivariate setting has remained limited.\n",
      "  In this work, we design a computationally efficient algorithm achieving an\n",
      "information-theoretically near-optimal error. Specifically, we show that the\n",
      "optimal error (up to polylogarithmic factors) is $f(\\alpha,N) + \\sqrt{D/(\\alpha\n",
      "N)}$, where the term $f(\\alpha,N)$ is the error of the one-dimensional problem\n",
      "and the second term is the sub-Gaussian error rate. Our algorithmic approach\n",
      "employs an iterative refinement strategy, whereby we progressively learn more\n",
      "accurate approximations $\\hat \\mu$ to $\\mu$. This is achieved via a novel\n",
      "rejection sampling procedure that removes points significantly deviating from\n",
      "$\\hat \\mu$, as an attempt to filter out unusually noisy samples. A complication\n",
      "that arises is that rejection sampling introduces bias in the distribution of\n",
      "the remaining points. To address this issue, we perform a careful analysis of\n",
      "the bias, develop an iterative dimension-reduction strategy, and employ a novel\n",
      "subroutine inspired by list-decodable learning that leverages the\n",
      "one-dimensional result.\n",
      "2025-01-09 18:31:35+00:00\n",
      "[arxiv.Result.Author('Ilias Diakonikolas'), arxiv.Result.Author('Daniel M. Kane'), arxiv.Result.Author('Sihan Liu'), arxiv.Result.Author('Thanasis Pittas')]\n",
      "http://arxiv.org/abs/2501.05425v1\n",
      "---\n",
      "Nebular emission from composite star-forming galaxies -- I. A novel modelling approach\n",
      "We introduce a novel approach to modelling the nebular emission from\n",
      "star-forming galaxies by combining the contributions from many HII regions\n",
      "incorporating loose trends in physical properties, random dust attenuation, a\n",
      "predefined Halpha luminosity function and a diffuse ionized-gas component.\n",
      "Using a machine-learning-based regression artificial neural network trained on\n",
      "a grid of models generated by the photoionization code Cloudy, we efficiently\n",
      "predict emission-line properties of individual HII regions over a wide range of\n",
      "physical conditions. We generate 250,000 synthetic star-forming galaxies\n",
      "composed of up to 3000 HII regions and explore how variations in parameters\n",
      "affect their integrated emission-line properties. Our results highlight\n",
      "systematic biases in oxygen-abundance estimates derived using traditional\n",
      "methods, emphasizing the importance of accounting for the composite nature of\n",
      "star-forming galaxies when interpreting integrated nebular emission. Future\n",
      "work will leverage this approach to explore in detail its impact on parameter\n",
      "estimates of star-forming galaxies.\n",
      "2025-01-09 18:31:29+00:00\n",
      "[arxiv.Result.Author('Christophe Morisset'), arxiv.Result.Author('Stéphane Charlot'), arxiv.Result.Author('Sebastián F. Sánchez'), arxiv.Result.Author('Carlos Espinosa-Ponce'), arxiv.Result.Author('Eric Barat'), arxiv.Result.Author('Thomas Dautreme')]\n",
      "http://arxiv.org/abs/2501.05424v1\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query='machine learning',\n",
    "  max_results=10,\n",
    "  sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "\n",
    "\n",
    "for r in client.results(search):\n",
    "    print(r.title)\n",
    "    print(r.summary)\n",
    "    print(r.published)\n",
    "    print(r.authors)\n",
    "    print(r.entry_id)\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
